{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example notebook that causes the largest OOI.pangeo.oi server to run out of memory when trying to lazily load and concatenate AMPS netcdfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import fsspec\n",
    "import gcsfs\n",
    "from tqdm import tqdm\n",
    "xr.set_options(display_style=\"html\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list the netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='ldeo-glaciology', mode='ab', cache_timeout = 0)\n",
    "NCs = fs.ls('gs://ldeo-glaciology/AMPS/wrf_d03_20161222_week-cf')\n",
    "len(NCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all the netcdf files in the AMPS directory and conat each one.\n",
    "This cell causes the kernel to restart when you try to loop over all the netcdfs (len (NCs) = 55) we currently have in GCS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 43/53 [03:00<03:56, 23.69s/it]"
     ]
    }
   ],
   "source": [
    "## load the first file to inialize the xarray\n",
    "url = 'gs://' + NCs[0]\n",
    "with  fsspec.open(url, mode='rb')  as openfile:  \n",
    "    AMPS = xr.open_dataset(openfile, chunks={})  # these chunk sizes produce chunks of reasonable data volumes and which stretch through all time\n",
    "\n",
    "## load the other files, each time concaternating them onto an xarray (AMPS) that grows in the time dimension each iteration. \n",
    "for i in tqdm(range(1, len(NCs)-1)):  \n",
    "    url = 'gs://' + NCs[i]\n",
    "    with  fsspec.open(url, mode='rb')  as openfile:  \n",
    "        temp = xr.open_dataset(openfile, chunks={})  # these chunk sizes produce chunks of reasonable data volumes and which stretch through all time\n",
    "    AMPS = xr.concat([AMPS,temp],'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am interested to know why the cell above fills the memory, when my intention was to only be loading things lazily. Note that when I load a smaller number of files (e.g., 45), instead of the full 55, I produce an xarray which seems to be made up of dask arrays as intended, so I dont why does it take up so much space on disk? Is it something to do with the chunking? increasing chunk size with AMPS.chunk doesnt seems to change the memory usage (monitored with top). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
